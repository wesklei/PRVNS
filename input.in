/*Input file*/
RUN = 5
MAX_AVAL = 1
POP_SIZE = 10
DIM = 250
FUNCTION = 0
KMAX = 5
TMAX = 500000
Q = 2.8
RADII = 0.1
RADII_T_FLAG = 1
AVAL_MAX = 20
RHO = 0.5
EPSILON = 1E-4
METHOD = 6
P=0
VNS_POP = 50
ECO_STEP = 5
EVO_STEP = 100
G_MAX = 10000
PC = 0.9


/* RUN is the number of times the ECO alg will run.*//*{{{*/
/* POP_SIZE is the number of candidate solutions that compounds each population. */
/* DIM is the number of problem variables. */
/* FUNCTION represents the problem to be solved. The current options are:*/
		/* .0: Rastrigin  		[  -5.12,   5.12]*//*{{{*/
		/*. 1: Schaffer   		[-100.00, 100.00]*/
		/*. 2: Griewank   		[-600.00, 600.00]*/
		/*. 3: Ackley     		[ -32.00,  32.00]*/
		/*. 4: Rosenbrock 		[ -30.00,  30.00]*/
		/*. 5: Sphere     		[-100.00, 100.00]*/
		/*6: Molecular Potential Energy [   0.00,   5.00]*/
		/*7: Schaffer F6		[-100.00, 100.00]*/
		/*8: GEneralized Schwefels	[-500.00, 500.00]*/
		/*12: Levy Function		[ -10.00,  10.00]*/
		/*13: Zakharov  		[  -5.00,  10.00]*/
		/*14: Egg Holder		[-512.00, 512.00]*/
		/*15: Holzman			[ -10.00,  10.00]*/
		/*16: Michalewitz		[   0.00,     PI]*/
		/*18: Powell			[ -10.00,  10.00]*/
		/*19: Rana			[-512.00, 512.00]*/
		/*20: Shubert			[  -4.00,   5.00]*//*}}}*/
		/*. 77: Schwefel's function 2.22 [-10, 10]*/

			
	/****RVNS PARAMETERS****/
/*K is the number of neighbourhood*/
/*TMAX is the number os iterations*/
/*Q is the geometric progresion rate*/
/*RADII is the radio aplied*/
/*RADII_T_FLAG is the flag for radii type test:
	0 => 	r[k-1] < pk < r[k]*/
	1 => 	pk < r[k]*/
/*METHOD	the algorithm to run*/
/*	2 =>	HJ*/
/*	3 =>	BVNS*/
/*	4 =>	RVNS*/
/*	5 =>	BVNS_NM */
/*	6 =>	PRVNS*/
/*	7 =>	NM*/

/*P the metric used:*/
	/*0 =>	l_inf (shebchev)*/
	/*1 =>	l1	(manhathan)*/
	/*2 =>	l2	(euclidian)*/

	/****HOOKE AND JEEVES PARAMETERS****//*{{{*/
/*ITERMAX = */
/*RHO = */
/*EPSILON = */
/*     rho	   {a double}  This is a user-supplied convergence */
/*		   parameter (more detail below), which should be  */
/*		   set to a value between 0.0 and 1.0.	Larger	   */
/*		   values of rho give greater probability of	   */
/*		   convergence on highly nonlinear functions, at a */
/*		   cost of more function evaluations.  Smaller	   */
/*		   values of rho reduces the number of evaluations */
/*		   (and the program running time), but increases   */
/*		   the risk of nonconvergence.	See below.	   */
/*     epsilon	   {a double}  This is the criterion for halting   */
/*		   the search for a minimum.  When the algorithm   */
/*		   begins to make less and less progress on each   */
/*		   iteration, it checks the halting criterion: if  */
/*		   the stepsize is below epsilon, terminate the    */
/*		   iteration and return the current best estimate  */
/*		   of the minimum.  Larger values of epsilon (such */
/*		   as 1.0e-4) give quicker running time, but a	   */
/*		   less accurate estimate of the minimum.  Smaller */
/*		   values of epsilon (such as 1.0e-7) give longer  */
/*		   running time, but a more accurate estimate of   */
/*		   the minimum. 				   */
/*     itermax	   {an integer}  A second, rarely used, halting    */
/*		   criterion.  If the algorithm uses >= itermax    */
/*		   iterations, halt.				   */

/* rho, the algorithm convergence control			   */
/*	The algorithm works by taking "steps" from one estimate of */
/*    a minimum, to another (hopefully better) estimate.  Taking   */
/*    big steps gets to the minimum more quickly, at the risk of   */
/*    "stepping right over" an excellent point.  The stepsize is   */
/*    controlled by a user supplied parameter called rho.  At each */
/*    iteration, the stepsize is multiplied by rho  (0 < rho < 1), */
/*    so the stepsize is successively reduced.			   */
/*	Small values of rho correspond to big stepsize changes,    */
/*    which make the algorithm run more quickly.  However, there   */
/*    is a chance (especially with highly nonlinear functions)	   */
/*    that these big changes will accidentally overlook a	   */
/*    promising search vector, leading to nonconvergence.	   */
/*	Large values of rho correspond to small stepsize changes,  */
/*    which force the algorithm to carefully examine nearby points */
/*    instead of optimistically forging ahead.	This improves the  */
/*    probability of convergence.				   */
/*	The stepsize is reduced until it is equal to (or smaller   */
/*    than) epsilon.  So the number of iterations performed by	   */
/*    Hooke-Jeeves is determined by rho and epsilon:		   */
/*	    rho**(number_of_iterations) = epsilon		   */
/*	In general it is a good idea to set rho to an aggressively */
/*    small value like 0.5 (hoping for fast convergence).  Then,   */
/*    if the user suspects that the reported minimum is incorrect  */
/*    (or perhaps not accurate enough), the program can be run	   */
/*    again with a larger value of rho such as 0.85, using the	   */
/*    result of the first minimization as the starting guess to    */
/*    begin the second minimization.				   *//*}}}*/
